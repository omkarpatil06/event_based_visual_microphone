{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-based Visual Microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "This notebook is designed to extract audible sounds from a video of an object vibrating in response to those sounds. The process begins with the transformation of a standard RGB video into an event video. This step is necessary because an event camera was not available at the time of this project. Following the transformation, the event video is then processed to recover sound.\n",
    "\n",
    "### Objectives\n",
    "- To break a RGB video into segments.\n",
    "- To convert the RGB videos to event videos, using a v2e toolbox inspired simulator.\n",
    "- To convert the event videos to sound using Abe Davis' Visual Microphone method.\n",
    "- To use a bandwidth extension model to enhance the recovered sound.\n",
    "- To visualise the recovered signal.\n",
    "\n",
    "### Dependencies\n",
    "To run this notebook, you will need the following libraries:\n",
    "- `cv2`: OpenCV\n",
    "- `numpy` : NumPy\n",
    "- `scipy` : SciPy\n",
    "- `torch` : PyTorch\n",
    "- `librosa` : Librosa\n",
    "- `tensorflow` : TensorFlow\n",
    "- `matplotlib` : Matplotlib\n",
    "- `soundfile` : Soundfile\n",
    "- `sounddevice` : Sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.video_frames as frames\n",
    "import event_camera.simulator_utils as camera\n",
    "import steerable_pyramid.davis_method as pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break RGB video into segments\n",
    "The code bellow assess the video size and breaks it into 2GB segments. The videos are stored in a tempory folder in Documents, which is then deleted automatically at the end.\n",
    "\n",
    "<span style=\"color:red\"> Enter the input video file path and fps bellow: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "framerate = 2200\n",
    "video_path = '/Volumes/Omkar 5T/video_dataset/plants.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.extract_video_segments(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert RGB video to event video\n",
    "This process uses a v2e toolbox inspired model created by Tobi Delbruck, Yuhuang Hu and Zhe He. The only difference is that our model doesn't include event noise. There are two important variables:\n",
    "- **Cut-off frequency** (`cutoff_freq`): for event pixel bandwidth: more information can be extracted at lower frequencies. This makes it perfect for low light conditions.\n",
    "- **+/-ve thresholds** (`pos_thresh` and `neg_thresh`) : completely based on cut-off frequency and the maximum allowable events for any pixel.\n",
    "\n",
    "### Ideal paramters for Visual Microphone dataset\n",
    "For the slow-motion RGB videos from Abe Davis' Visual Microphone dataset, the ideal parameters are:\n",
    "- MIDI Chips Bag: `3e-4` cutoff-freq, `8e-7` pos_thresh and `8e-7` neg_thresh\n",
    "- MIDI Plants: `3e-5` cutoff-freq, `9e-8` pos_thresh and `9e-8` neg_thresh\n",
    "- Speech Chips Bag 2.2kHz:\n",
    "- Speech Chips Bag 20kHz:\n",
    "\n",
    "<span style=\"color:red\"> Enter the simulation parameters bellow: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_freq = 3e-5\n",
    "pos_thresh = 9e-8\n",
    "neg_thresh = 9e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fps = 30\n",
    "sampling_period = 1/video_fps\n",
    "camera.event_simulator(sampling_period, cutoff_freq, pos_thresh, neg_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow allows to see the event video after conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert event video to sound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uses a phase-based method. The method applies a steerable pyramid to get a phase response at several scales and orientations. Then several steps are applied to flatten and average the response to a time-series signal. The important parameters for this process are:\n",
    "- **Number of Scales** (`nscales`): defines the number of levels of the pyramid. For visual microphone this was set to 2.\n",
    "- **Number of Orientations** (`norientations`): defines the number of steerable filters at a level of the pyramid. For visual microphone this was set to 4.\n",
    "\n",
    "<span style=\"color:red\"> Enter the steerable pyramid parameters and recovered sound file path bellow: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nscales = 2\n",
    "norientations = 4\n",
    "save_path = '/Volumes/Omkar 5T/chips1.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes several hours, traditionally 2hrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid.ebvmSoundfromVideo(save_path, nscales, norientations, framerate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
